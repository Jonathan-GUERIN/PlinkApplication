{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3da9a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_repo' from 'huggingface_hub' (C:\\Users\\jonat\\anaconda3\\envs\\fract\\lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20120/2308796952.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_cosine_schedule_with_warmup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fract\\lib\\site-packages\\transformers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m from .file_utils import (\n\u001b[0;32m     32\u001b[0m     \u001b[0m_LazyModule\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fract\\lib\\site-packages\\transformers\\dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpkg\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"tokenizers\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# must be loaded here, or else tqdm check may fail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fract\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfilelock\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFileLock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRepository\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_repo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_repo_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhoami\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'create_repo' from 'huggingface_hub' (C:\\Users\\jonat\\anaconda3\\envs\\fract\\lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.special import softmax\n",
    "import cv2\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "from albumentations import *\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93244c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_healthy = plt.imread('./data/plant-pathology-2020-fgvc7/images/Train_2.jpg', format = 'jpg')\n",
    "im_multi = plt.imread('./data/plant-pathology-2020-fgvc7/images/Train_1.jpg', format = 'jpg')\n",
    "im_rust = plt.imread('./data/plant-pathology-2020-fgvc7/images/Train_3.jpg', format = 'jpg')\n",
    "im_scab = plt.imread('./data/plant-pathology-2020-fgvc7/images/Train_0.jpg', format = 'jpg')\n",
    "\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "ax.imshow(im_healthy)\n",
    "ax.set_title('Healthy', fontsize = 20)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "ax.imshow(im_multi)\n",
    "ax.set_title('Multiple Diseases', fontsize = 20)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "ax.imshow(im_rust)\n",
    "ax.set_title('Rust', fontsize = 20)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "ax.imshow(im_scab)\n",
    "ax.set_title('Scab', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "IMAGE_FOLDER = '../data/plant-pathology-npy-images/images/'\n",
    "\n",
    "def get_image_path(filename):\n",
    "    return (IMAGE_FOLDER + filename + '.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train = pd.read_csv('../input/plant-pathology-2020-fgvc7/train.csv')\n",
    "test = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')\n",
    "\n",
    "train['image_path'] = train['image_id'].apply(get_image_path)\n",
    "test['image_path'] = test['image_id'].apply(get_image_path)\n",
    "train_labels = train.loc[:, 'healthy':'scab']\n",
    "train_paths = train.image_path\n",
    "test_paths = test.image_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2508a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, train_labels, test_size = 0.2, random_state=23, stratify = train_labels)\n",
    "train_paths.reset_index(drop=True,inplace=True)\n",
    "train_labels.reset_index(drop=True,inplace=True)\n",
    "valid_paths.reset_index(drop=True,inplace=True)\n",
    "valid_labels.reset_index(drop=True,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ab279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafDataset(Data.Dataset):\n",
    "    def __init__(self, image_paths, labels = None, train = True, test = False):\n",
    "        self.paths = image_paths\n",
    "        self.test = test\n",
    "        if self.test == False:\n",
    "            self.labels = labels\n",
    "        self.train = train\n",
    "        self.train_transform = Compose([HorizontalFlip(p=0.5),\n",
    "                                  VerticalFlip(p=0.5),\n",
    "                                  ShiftScaleRotate(rotate_limit=25.0, p=0.7),\n",
    "                                  OneOf([IAAEmboss(p=1),\n",
    "                                         IAASharpen(p=1),\n",
    "                                         Blur(p=1)], p=0.5),\n",
    "                                  IAAPiecewiseAffine(p=0.5)])\n",
    "        self.test_transform = Compose([HorizontalFlip(p=0.5),\n",
    "                                       VerticalFlip(p=0.5),\n",
    "                                       ShiftScaleRotate(rotate_limit=25.0, p=0.7)])\n",
    "        self.default_transform = Compose([Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), always_apply=True),\n",
    "                                         ToTensor()]) #normalized for pretrained network\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.paths.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = np.load(self.paths[i]) #load from .npy file!\n",
    "        if self.test==False:\n",
    "            label = torch.tensor(np.argmax(self.labels.loc[i,:].values)) #loss function used later doesnt take one-hot encoded labels, so convert it using argmax\n",
    "        if self.train:\n",
    "            image = self.train_transform(image=image)['image']\n",
    "            image = self.default_transform(image=image)['image']\n",
    "        elif self.test:\n",
    "            image = self.test_transform(image=image)['image']\n",
    "            image = self.default_transform(image=image)['image']\n",
    "        else:\n",
    "            image = self.default_transform(image=image)['image']\n",
    "        \n",
    "        if self.test==False:\n",
    "            return image, label\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(net, loader):\n",
    "    \n",
    "    running_loss = 0\n",
    "    preds_for_acc = []\n",
    "    labels_for_acc = []\n",
    "    \n",
    "    pbar = tqdm(total = len(loader), desc='Training')\n",
    "    \n",
    "    for _, (images, labels) in enumerate(loader):\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = net(images)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()*labels.shape[0]\n",
    "        labels_for_acc = np.concatenate((labels_for_acc, labels.cpu().numpy()), 0)\n",
    "        preds_for_acc = np.concatenate((preds_for_acc, np.argmax(predictions.cpu().detach().numpy(), 1)), 0)\n",
    "        \n",
    "        pbar.update()\n",
    "        \n",
    "    accuracy = accuracy_score(labels_for_acc, preds_for_acc)\n",
    "    \n",
    "    pbar.close()\n",
    "    return running_loss/TRAIN_SIZE, accuracy\n",
    "\n",
    "def valid_fn(net, loader):\n",
    "    \n",
    "    running_loss = 0\n",
    "    preds_for_acc = []\n",
    "    labels_for_acc = []\n",
    "    \n",
    "    pbar = tqdm(total = len(loader), desc='Validation')\n",
    "    \n",
    "    with torch.no_grad():       #torch.no_grad() prevents Autograd engine from storing intermediate values, saving memory\n",
    "        for _, (images, labels) in enumerate(loader):\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            net.eval()\n",
    "            predictions = net(images)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            \n",
    "            running_loss += loss.item()*labels.shape[0]\n",
    "            labels_for_acc = np.concatenate((labels_for_acc, labels.cpu().numpy()), 0)\n",
    "            preds_for_acc = np.concatenate((preds_for_acc, np.argmax(predictions.cpu().detach().numpy(), 1)), 0)\n",
    "            \n",
    "            pbar.update()\n",
    "            \n",
    "        accuracy = accuracy_score(labels_for_acc, preds_for_acc)\n",
    "        conf_mat = confusion_matrix(labels_for_acc, preds_for_acc)\n",
    "    \n",
    "    pbar.close()\n",
    "    return running_loss/VALID_SIZE, accuracy, conf_mat\n",
    "\n",
    "def test_fn(net, loader):\n",
    "\n",
    "    preds_for_output = np.zeros((1,4))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total = len(loader))\n",
    "        for _, images in enumerate(loader):\n",
    "            images = images.to(device)\n",
    "            net.eval()\n",
    "            predictions = net(images)\n",
    "            preds_for_output = np.concatenate((preds_for_output, predictions.cpu().detach().numpy()), 0)\n",
    "            pbar.update()\n",
    "    \n",
    "    pbar.close()\n",
    "    return preds_for_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef80da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 30\n",
    "TRAIN_SIZE = train_labels.shape[0]\n",
    "VALID_SIZE = valid_labels.shape[0]\n",
    "MODEL_NAME = 'efficientnet-b5'\n",
    "device = 'cuda'\n",
    "lr = 8e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LeafDataset(train_paths, train_labels)\n",
    "trainloader = Data.DataLoader(train_dataset, shuffle=True, batch_size = BATCH_SIZE, num_workers = 2)\n",
    "\n",
    "valid_dataset = LeafDataset(valid_paths, valid_labels, train = False)\n",
    "validloader = Data.DataLoader(valid_dataset, shuffle=False, batch_size = BATCH_SIZE, num_workers = 2)\n",
    "\n",
    "test_dataset = LeafDataset(test_paths, train = False, test = True)\n",
    "testloader = Data.DataLoader(test_dataset, shuffle=False, batch_size = BATCH_SIZE, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a731e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_pretrained(MODEL_NAME)\n",
    "\n",
    "num_ftrs = model._fc.in_features\n",
    "model._fc = nn.Sequential(nn.Linear(num_ftrs,1000,bias=True),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.5),\n",
    "                          nn.Linear(1000,4, bias = True))\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr = lr, weight_decay = 1e-3)\n",
    "num_train_steps = int(len(train_dataset) / BATCH_SIZE * NUM_EPOCHS)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_dataset)/BATCH_SIZE*5, num_training_steps=num_train_steps)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d726be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_acc = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6bdb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    tl, ta = train_fn(model, loader = trainloader)\n",
    "    vl, va, conf_mat = valid_fn(model, loader = validloader)\n",
    "    train_loss.append(tl)\n",
    "    valid_loss.append(vl)\n",
    "    train_acc.append(ta)\n",
    "    val_acc.append(va)\n",
    "    \n",
    "    if (epoch+1)%10==0:\n",
    "        path = 'epoch' + str(epoch) + '.pt'\n",
    "        torch.save(model.state_dict(), path)\n",
    "    \n",
    "    printstr = 'Epoch: '+ str(epoch) + ', Train loss: ' + str(tl) + ', Val loss: ' + str(vl) + ', Train acc: ' + str(ta) + ', Val acc: ' + str(va)\n",
    "    tqdm.write(printstr)\n",
    "    \n",
    "'''Output hidden. Unhide to see training log'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.ylim(0,1.5)\n",
    "#sns.lineplot(list(range(len(train_loss))), train_loss)\n",
    "#sns.lineplot(list(range(len(valid_loss))), valid_loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train','Val'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aceadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#sns.lineplot(list(range(len(train_acc))), train_acc)\n",
    "#sns.lineplot(list(range(len(val_acc))), val_acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train','Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labels = ['Healthy', 'Multiple','Rust','Scab']\n",
    "#sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = []\n",
    "for i in range(5): #average over 5 runs\n",
    "    out = test_fn(model, testloader)\n",
    "    output = pd.DataFrame(softmax(out,1), columns = ['healthy','multiple_diseases','rust','scab']) #the submission expects probability scores for each class\n",
    "    output.drop(0, inplace = True)\n",
    "    output.reset_index(drop=True,inplace=True)\n",
    "    subs.append(output)\n",
    "\n",
    "sub_eff1 = sum(subs)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub_eff1.copy()\n",
    "sub1['image_id'] = test.image_id\n",
    "sub1 = sub1[['image_id','healthy','multiple_diseases','rust','scab']]\n",
    "sub1.to_csv('submission_efficientnet1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f39fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "del model\n",
    "del optimizer\n",
    "del scheduler\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
